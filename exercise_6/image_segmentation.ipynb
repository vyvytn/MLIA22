{"cells":[{"cell_type":"markdown","metadata":{"id":"UzTxVVaw4opK"},"source":["# Introduction\n","\n","In the last exercises, you implemented different networks for image classification on the CIFAR10 Dataset.\n","For this exercise you will work on pixelwise segmentation (think: classification per pixel instead of per image). \n","There are multiple types of segmentation, the most prominent ones are semantic segmentation and instance segmentation.\n","The goal of semantic segmentation is to assign to each pixel a class (e.g. car, bike), whereas the goal of instance segmentation is to assign to each pixel some object id.\n","This can also be combined to e.g. extract all individual cars from an image (semantic instance segmentation)."]},{"cell_type":"markdown","source":["## General Information\n","\n","**Workflow** For this assignment you need a GPU. If you have one, you can keep on working locally with this notebook. If not, we recommend to use colab. If both options do not work for you, please let us know.\n","\n","**Assignment submission** You need to submit both this notebook and the model(s) you've trained. If the model is small enough you can submit it via moodle, otherwise upload it e.g. on google drive or firefox send and submit a text file with the link to it."],"metadata":{"id":"Z8MCBe4hxuud"}},{"cell_type":"markdown","source":["## How to setup Google Colab\n","\n","You already know about colab from the previous exercises. Here are the most important infos again:\n","\n","**Requirements** To use Colab, you must have a Google account with an associated Google Drive.\n","\n","**Reminder** Ressources on colab are not guaranteed and therefore there might be times where some ressources cannot get allocated. If you're idle for 90 minutes or your connection time exceeds the maximum of 12 hours, the colab virtual machine will disconnect. This means that unsaved progress such as model parameters are lost.\n","\n","**Upload the data**\n","You need to upload your data on the remote machine. Make sure to upload the zip and not the extracted data, this is significantly faster. There are two primary options:  \n","You can upload the data directly into colab (Click on the Files icon on the left side and then on upload). This is the most straightforward way, but you have to do it every time you start a new colab session.  \n","The second option is to use Google Drive and import the data from there into Colab. \n","First, you need to upload the provided zip file (data.zip) to your Google Drive. Next, you mount your Google Drive on the remote machine. In order to do so, you can use the cell below.  \n","In all case you then have to execute the \"extract data\" cell to unpack the zip file (You might have to change the path_to_zip variable).  \n","Use the \"verify\" cell to make sure that the data is accessible."],"metadata":{"id":"3SjM6uPLx29b"}},{"cell_type":"code","source":["### mount your google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# add your local project folder to be able to import all utils files\n","import os\n","path = \"/content/drive/My Drive/Colab Notebooks/sose22\" # change this to your local project folder\n","os.chdir(path)"],"metadata":{"id":"OJ46K5nFXYAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLaWOhPF4opR"},"outputs":[],"source":["path_to_zip = \"data.zip\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BiiUNAk4opS"},"outputs":[],"source":["!unzip $path_to_zip"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"W9_bDN6qKyRm"},"outputs":[],"source":["# verify that the data is found\n","import glob\n","fls = glob.glob(os.path.join(\"dsb2018\", \"train\", \"*.zarr\"))\n","print(len(fls), fls[:5])"]},{"cell_type":"markdown","metadata":{"id":"NSIXR4QG4opV"},"source":["## Grading\n","\n","This exercise counts 30% towards your total score.\n","You can get a maximum of 100 points.\n","\n","| Section | Part | Task | Points |\n","|------|----------------------------------------|-------------|-------------|\n","| 1 | Data Loader | Augmentation                             | 10          |\n","| 2 | U-Net | Implementation           | 30          |\n","| 3 | Foreground/Background Segmentation | Calculate accuracy   | 5          |\n","|  | | Call train        | 5          |\n","| 4 |Receptive Field| Implementation                       | 10          |\n","| ||Question 1                                            | 5          |\n","| 5 | Instance Segmentation | Define losses   | 10          |\n","| || Define U-Net                                |  5          | \n","| || Call train                                |  5          | \n","| ||Benchmark                                            | 10          |\n","| ||Question 2                                            |5          |\n","| |Total Points|                                            | 100          |"]},{"cell_type":"markdown","source":["## Install and import packages"],"metadata":{"id":"QlYTjbG3zZKR"}},{"cell_type":"code","source":["!pip uninstall albumentations -y\n","!pip install numpy pillow matplotlib scikit-image zarr imgaug==0.4.0 torchsummary tensorboard mahotas"],"metadata":{"id":"6ryvpqZWbnP8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myvzio470JgQ","scrolled":true},"outputs":[],"source":["%matplotlib inline\n","from IPython.core.display import display, HTML\n","# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n","import os\n","#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from __future__ import division, print_function\n","import glob\n","import zarr\n","import random\n","import datetime\n","import numpy as np\n","import math\n","\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from imgaug import augmenters as iaa\n","from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n","from imgaug.augmentables.heatmaps import HeatmapsOnImage\n","\n","from skimage import io\n","from utils.colormap import *\n","from utils.mean_shift import MeanShift\n","\n","plt.rcParams['image.cmap'] = 'gist_earth'\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","metadata":{"id":"UjIjCh064opW"},"source":["# Data\n","For this task we use a subset of the data used in the kaggle data science bowl 2018 challenge (https://www.kaggle.com/c/data-science-bowl-2018/).\n","\n","Example image:\n","\n","![kaggle_example](https://data.broadinstitute.org/bbbc/BBBC038/BBBC038exampleimage1.png)\n","\n","All images show nuclei recorded using different microscopes and lighting conditions. There are 30 images in the training set, 8 in the validation set and 16 in the test set."]},{"cell_type":"markdown","source":["Visualize the data:"],"metadata":{"id":"icPj7IUBQRFY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-qPtd_7K0u-"},"outputs":[],"source":["# utility functions to visualize data\n","\n","def plot_image(raw, labels, pred=None):\n","    fig=plt.figure(figsize=(12, 8))\n","    if pred is not None:\n","        num_plots = 3\n","    else:\n","        num_plots = 2\n","    fig.add_subplot(1, num_plots, 1)\n","    plt.imshow(np.squeeze(raw), cmap='gray')\n","    fig.add_subplot(1, num_plots, 2)\n","    plt.imshow(np.squeeze(labels), cmap='gist_earth')\n","    if pred is not None:\n","        fig.add_subplot(1, num_plots, 3)\n","        plt.imshow(np.squeeze(pred), cmap='gist_earth')\n","    \n","def plot_random_image():\n","    fls = glob.glob(os.path.join(\"dsb2018\", \"train\", \"*.zarr\"))\n","    fl = zarr.open(fls[random.randrange(len(fls))], 'r')\n","    raw = fl[\"volumes/raw\"]\n","    labels = fl[\"volumes/gt_threeclass\"]\n","    plot_image(raw, labels)"]},{"cell_type":"markdown","metadata":{"id":"mqbRK7puK0vA"},"source":["Execute the next cell repeatedly to have a look at the data. \n","We start with the foreground/background segmentation. So the ground truth has 2 labels: Background is zero and cell is one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBRq18PRK0vB"},"outputs":[],"source":["# repeatedly execute this cell to get different images\n","plot_random_image()"]},{"cell_type":"markdown","metadata":{"id":"2S-rW5nJK0vD"},"source":["## Load the data\n","\n","Here we implement a custom dataset for the kaggle data. For more information, you can look here:\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"rarEgU1iK0vE"},"source":["### todo: Augmentation\n","\n","Especially when the size of the dataset is limited, data augmentation is essential to get good results.\n","Extend your data loader to augment your data during training on the fly.\n","Think about what kind of augmentation to use (e.g. flips, rotation, elastic) and if you might need to pad or crop your data.\n","\n","We have prepared the code to use the external data augmentation library imgaug (https://imgaug.readthedocs.io/en/latest/), but you are free to change the code to implement the data augmentation differently when you like (e.g. using numpy or torchvision.transfom functions)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk-JkfTdK08A"},"outputs":[],"source":["class KaggleDSB_dataset(Dataset):\n","    \"\"\"(subset of the) kaggle data science bowl 2018 dataset.\n","    The data is loaded from disk on the fly and in parallel using the torch dataset class.\n","    This enables the use of datasets that would not fit into main memory and dynamic augmentation.\n","    Args:\n","        root_dir (string): Directory with all the images.\n","        mode (string): train/val/test, select subset of images\n","        prediction_type (string): default to be \"metric_learning\" for this notebook\n","        net_input_size (list): the input title size of you UNet\n","        padding_size (int): the number of pixels to pad on each side of the image before augmentation and cropping\n","        cache: if cache the data, default: False\n","    \"\"\"\n","    def __init__(self,\n","                 root_dir,\n","                 mode,\n","                 prediction_type=\"two_class\",\n","                 net_input_size=None,\n","                 padding_size=None\n","                ):\n","        self.mode = mode\n","        self.files = glob.glob(os.path.join(root_dir, mode, \"*.zarr\"))\n","        self.prediction_type = prediction_type\n","        self.net_input_size = net_input_size\n","        self.padding_size = padding_size\n","        self.define_augmentation()\n","\n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def define_augmentation(self):\n","        \n","        self.transform = iaa.Identity\n","        self.crop = None\n","        self.pad = None\n","\n","        ###########################################################################\n","        # TODO: Define your augmentation pipeline and uncomment the    #\n","        # following code                                                          #\n","        ###########################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        \n","        # define self.transfrom by looking into the imgaug package reference\n","        \n","        # self.transform = iaa.Sequential([\n","        #     ...,\n","        #     ...,\n","        #    ...\n","        # ], random_order=True)\n","        \n","        \n","        # if self.net_input_size is not None:\n","        #     self.crop = ...\n","\n","        # if self.padding_size is not None:\n","        #     self.pad =  ...\n","            \n","            \n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","        \n","    def get_filename(self, idx):\n","        return self.files[idx]\n","        \n","    def __getitem__(self, idx):\n","        fn = self.get_filename(idx)\n","        raw, label = self.load_sample(fn)\n","        raw = self.normalize(raw)\n","        # augment for training\n","        if self.padding_size is not None:\n","            raw = self.pad(images = raw) # CHW -> CHW\n","            label = self.pad(images = label) # CHW -> CHW\n","        if self.mode == \"train\":\n","            raw = np.transpose(raw, [1,2,0]) # CHW -> HWC\n","            label = np.transpose(label, [1,2,0]) # CHW -> HWC            \n","            raw, label = self.augment_sample(raw, label) # HWC -> HWC\n","            raw = np.transpose(raw, [2,0,1]) # HWC -> CHW\n","            label = np.transpose(label, [2,0,1]) # HWC -> CHW\n","        if self.net_input_size is not None:\n","            tmp = np.concatenate([raw, label], axis = 0).copy() # C1+C2 HW\n","            tmp = np.transpose(tmp, [1,2,0]) # CHW -> HWC \n","            tmp = self.crop.augment_image(tmp) # HWC -> HWC\n","            tmp = np.transpose(tmp, [2,0,1])\n","            raw, label = np.expand_dims(tmp[0], axis=0), np.stack(tmp[1:],axis=0) # split\n","        raw, label = torch.tensor(raw), torch.tensor(label)\n","        return raw, label\n","    \n","    def augment_sample(self, raw, label):\n","        # stores float label (sdt) differently than integer label (rest)\n","        if self.prediction_type in [\"sdt\"]:\n","            label = HeatmapsOnImage(label, shape=raw.shape, min_value=-1.0, max_value=1.0)\n","            raw, label = self.transform(image=raw, heatmaps=label)\n","        else:\n","            label = label.astype(np.int32)\n","            label = SegmentationMapsOnImage(label, shape=raw.shape)\n","            raw, label = self.transform(image=raw, segmentation_maps=label)\n","            \n","        label = label.get_arr() \n","        # some pytorch version have problems with negative indices introduced by e.g. flips\n","        # just copying fixes this\n","        label = label.copy()\n","        raw = raw.copy()\n","        return raw, label\n","    \n","    def normalize(self, raw):\n","        # z-normalization\n","        raw -= np.mean(raw)\n","        raw /= np.std(raw)\n","        return raw\n","    \n","    def load_sample(self, filename):\n","        data = zarr.open(filename)\n","        raw = np.array(data['volumes/raw'])\n","        if self.prediction_type == \"two_class\":\n","            label = np.array(data['volumes/gt_fgbg'])\n","        elif self.prediction_type == \"three_class\":\n","            label = np.array(data['volumes/gt_threeclass'])\n","        elif self.prediction_type == \"affinities\":\n","            label = np.array(data['volumes/gt_affs'])\n","        elif self.prediction_type == \"sdt\":\n","            label = np.array(data['volumes/gt_tanh'])\n","        elif self.prediction_type == \"metric_learning\":\n","            label = np.array(data['volumes/gt_labels'])\n","        label = label.astype(np.float32)\n","        return raw, label"]},{"cell_type":"markdown","metadata":{"id":"2Jtv8tSVK0vG"},"source":["### Check the data loader\n","Look at the training data to see if your dataset and data loader are working and,\n","if you are using augmentation, if the augmentation looks reasonable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9beWe9jaK0vG"},"outputs":[],"source":["dataset = \"dsb2018\"\n","prediction_type = \"two_class\"\n","num_fmaps_out = 2\n","\n","tmp_data = KaggleDSB_dataset(dataset,  mode=\"train\", prediction_type=prediction_type)\n","tmp_loader = DataLoader(tmp_data, batch_size=1, shuffle=True)\n","\n","dataiter = iter(tmp_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhNrcLVuK0vH"},"outputs":[],"source":["# repeatedly execute this cell\n","image, label = next(dataiter)\n","print(image.shape, label.shape)\n","plot_image(image, label)"]},{"cell_type":"markdown","metadata":{"id":"ia6m-VYqK0vH"},"source":["# U-Net\n","\n","The network you will use is a so-call U-Net. Your task is to implement it and use it to segment the dataset described above.\n","The U-Net is a popular core architecture for segmentation and extensively used for medical and biological data.\n","\n","![unet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n","\n","Start by reading the paper: [U-Net: Convolutional Networks for BiomedicalImage Segmentation](https://arxiv.org/pdf/1505.04597.pdf). It has been hugely influential in the field and it is very concise and understandable.\n","We recommend asking your colleagues on the moodle forums with respect to unclear aspects of the architecture.\n","(If that does not solve it, there are also a number of tutorials online)."]},{"cell_type":"markdown","source":["## todo: Implementation\n","\n","Your main task is to implement a U-Net yourselves. \n","(Disclaimer: We are aware that there are a number of implementations out there, but we **strongly recommend** to not look at them before trying it yourselves, this is a very fundamental exercise, and the knowledge you gain will be very helpful to you if you try to implement other architectures, and you learn significantly more by trying it yourselves than by looking at other people's code.\n","The implementation is not completely straight forward. If you are stuck, ask in the forum or check out one of the tutorials online.\n","Feel free to look at the implementations **after** you have a working version and get results on the data.\n","It might be a helpful exercise to check what they maybe have done better and what you maybe have done better.\n","Yet there is no need to change your implementation, there are no extra points for the most elegant or efficient solution).\n","\n","**Padding:** You can either use valid or same padding for your convolutions.\n","\n","**Upsampling**: Pytorch has modules for upsampling and transposed convolutions. Choose whatever you like to try out."],"metadata":{"id":"kK0g7nf5NvPj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cF6CpBJpSBr7"},"outputs":[],"source":["class UNet(torch.nn.Module):\n","\n","    def __init__(\n","            self,\n","            in_channels,\n","            num_fmaps,\n","            fmap_inc_factors,\n","            downsample_factors,\n","            kernel_size_down=None,\n","            kernel_size_up=None,\n","            activation='ReLU',\n","            padding='VALID', # might not be needed\n","            num_fmaps_out=None,\n","            constant_upsample=True # might not be needed\n","            ):\n","        '''Create a U-Net::\n","\n","            f_in --> f_left --------------------------->> f_right--> f_out\n","                        |                                   ^\n","                        v                                   |\n","                     g_in --> g_left ------->> g_right --> g_out\n","                                 |               ^\n","                                 v               |\n","                                       ...\n","\n","        where each ``-->`` is a convolution pass, each `-->>` a crop, and down\n","        and up arrows are max-pooling and transposed convolutions,\n","        respectively.\n","\n","        The U-Net expects 2D tensors shaped like::\n","\n","            ``(batch=1, channels, height, width)``.\n","\n","        This U-Net performs only \"valid\" convolutions, i.e., sizes of the\n","        feature maps decrease after each convolution.\n","\n","        Args:\n","\n","            in_channels:\n","\n","                The number of input channels.\n","\n","            num_fmaps:\n","\n","                The number of feature maps in the first layer. This is also the\n","                number of output feature maps. Stored in the ``channels``\n","                dimension.\n","\n","            fmap_inc_factors:\n","\n","                By how much to multiply the number of feature maps between\n","                layers. If layer 0 has ``k`` feature maps, layer ``l`` will\n","                have ``k*fmap_inc_factor**l``.\n","\n","            downsample_factors:\n","\n","                List of tuples ``(y, x)`` to use to down- and up-sample the\n","                feature maps between layers.\n","\n","            kernel_size_down (optional):\n","\n","                List of lists of kernel sizes. The number of sizes in a list\n","                determines the number of convolutional layers in the\n","                corresponding level of the build on the left side. Kernel sizes\n","                can be given as tuples or integer. If not given, each\n","                convolutional pass will consist of two 3x3 convolutions.\n","\n","            kernel_size_up (optional):\n","\n","                List of lists of kernel sizes. The number of sizes in a list\n","                determines the number of convolutional layers in the\n","                corresponding level of the build on the right side. Within one\n","                of the lists going from left to right. Kernel sizes can be\n","                given as tuples or integer. If not given, each convolutional\n","                pass will consist of two 3x3 convolutions.\n","\n","            activation:\n","\n","                Which activation to use after a convolution. Accepts the name\n","                of any tensorflow activation function (e.g., ``ReLU`` for\n","                ``torch.nn.ReLU``).\n","\n","            fov (optional):\n","\n","                Initial field of view\n","\n","\n","            constant_upsample (optional):\n","\n","                If set to true, perform a constant upsampling instead of a\n","                transposed convolution in the upsampling layers.\n","        '''\n","\n","        super(UNet, self).__init__()\n","\n","        self.num_levels = len(downsample_factors) + 1\n","        self.in_channels = in_channels\n","        self.out_channels = num_fmaps_out if num_fmaps_out else num_fmaps\n","        self.constant_upsample = constant_upsample\n","\n","        # default arguments\n","        if kernel_size_down is None:\n","            kernel_size_down = [[(3, 3), (3, 3)]]*self.num_levels\n","        if kernel_size_up is None:\n","            kernel_size_up = [[(3, 3), (3, 3)]]*(self.num_levels - 1)\n","            \n","        self.kernel_size_down = kernel_size_down\n","        self.kernel_size_up = kernel_size_up\n","        self.downsample_factors = downsample_factors\n","\n","        ########################################################################\n","        # TODO:                                                                #\n","        # - Implement your U-Net class here                                    #\n","        # - (later) Add a function to compute its receptive field              #\n","        ########################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","        \n","        \n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################"]},{"cell_type":"markdown","source":["# Foreground Segmentation\n","Define your model for foreground/background segmentation and print out your network summary."],"metadata":{"id":"7pZWN2MRVf3B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"voDDzLeeQvXz"},"outputs":[],"source":["# set seed\n","torch.manual_seed(42)\n","\n","# define model\n","out_channels = 1\n","d_factors = [[2,2],[2,2],[2,2],[2,2]]\n","activation = torch.nn.Sigmoid()\n","\n","net = torch.nn.Sequential(\n","    UNet(in_channels=1,\n","    num_fmaps=6,\n","    fmap_inc_factors=2,\n","    downsample_factors=d_factors,\n","    activation='ReLU',\n","    padding='same', # 'valid' or 'same'\n","    num_fmaps_out=6,\n","    constant_upsample=False\n","    ),\n","    torch.nn.Conv2d(in_channels=6, out_channels=out_channels, kernel_size=1, padding=0, bias=True))\n","\n","# print network layers\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = net.to(device)\n","summary(net, (1, 384, 384)) # e.g. (1, 380, 380) for valid padding, (1, 384, 384) for same padding"]},{"cell_type":"markdown","metadata":{"id":"wkWClnELQvX3"},"source":["### todo: Train \n","In the training process of semantic segmentation scenario, we usally will record two basic criteria, loss and pixel accuracy.\n","The loss function we use here is the Binary Cross Entropy with the sigmoid function before it.\n","The pixel accuracy, or accuracy for short, refers to the percent of pixels in the image which were correctly classified."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QmVNUbwW8xH"},"outputs":[],"source":["# make datasets\n","dataset = \"dsb2018\"\n","padding_size = 12   # you might need to change this, e.g. 10 for valid padding, 12 for same padding\n","batch_size = 4\n","\n","data_train = KaggleDSB_dataset(dataset, \"train\", prediction_type=prediction_type, padding_size=padding_size)\n","data_val = KaggleDSB_dataset(dataset, \"val\", prediction_type=prediction_type, padding_size=padding_size)\n","data_test = KaggleDSB_dataset(dataset, \"test\", prediction_type=prediction_type, padding_size=padding_size)\n","\n","# make dataloaders\n","train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n","val_loader = DataLoader(data_val, batch_size=1, pin_memory=True)\n","test_loader = DataLoader(data_test, batch_size=1)"]},{"cell_type":"markdown","source":["1. Define the **calc_accuracy** function according to the activation parameter you define before.This function should calculate the pixel accuracy of the prediction results average on one batch.\n","\n","2. Call the **train** function to start the training."],"metadata":{"id":"HHFohouVd-vO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEcCdujmQvX4"},"outputs":[],"source":["###########################################################################\n","# TODO:  calc_accuracy function                                           #                     \n","###########################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","def calc_accuracy(y_pred, y_true):\n","    # Please remember to get the mean accuracy of one batch\n","    return \n","\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################"]},{"cell_type":"code","source":["# function to save the current state of your model\n","# save_model(epoch, net, optimizer, running_loss, \"model-{}.pth\".format(epoch))\n","def save_model(epoch, net, optimizer, loss, filename):\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': net.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss\n","        }, filename)\n","    \n","# function to crop label to the network output size\n","def get_cropping(in_size, out_size):\n","  # assume in_size and out_size with shape B x C x H x W\n","  diff_size = np.array(in_size[2:]) - np.array(out_size[2:])\n","  if np.any(diff_size > 0):\n","      slices = [slice(None)] * 2 + [slice(math.floor(i / 2), - math.ceil(i / 2)) for i in diff_size]\n","  else:\n","      slices = [slice(None)] * len(in_size)\n","  \n","  return slices"],"metadata":{"id":"s_AdVaWJu8ld"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPPKJa_x0Jge","scrolled":true},"outputs":[],"source":["dtype = torch.FloatTensor\n","def training_step(model, loss_fn, optimizer, feature, label):\n","    # speedup version of setting gradients to zero\n","    for param in model.parameters():\n","        param.grad = None\n","    # forward\n","    logits = model(feature) # B x C x H x W\n","    loss_value = loss_fn(input=logits, target=label) # label.squeeze(0) for three_class\n","    # backward if training mode\n","    if net.training:\n","        loss_value.backward()\n","        optimizer.step()\n","    if activation is not None:\n","        output = activation(logits)\n","    else:\n","        output = logits\n","    outputs = {\n","        'pred': output,\n","        'logits': logits,\n","    }\n","    return loss_value, outputs\n","\n","def train(net, epochs, learning_rate,start_epoch=0, optimizer=None, history=None):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    net = net.to(device)\n","    loss_fn = torch.nn.BCEWithLogitsLoss()\n","    loss_fn = loss_fn.to(device)\n","    # set optimizer\n","    if optimizer is None:\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    if history is None:\n","        history = {'loss':[],\n","                  'val_loss':[],\n","                  'binary_accuracy':[],\n","                  'val_binary_accuracy':[]}\n","    output_slices = None\n","   \n","    pbar = tqdm(total=epochs*len(train_loader))\n","    for epoch in range(0, epochs):\n","        # reset data loader to get random augmentations\n","        np.random.seed()\n","        tmp_loader = iter(train_loader)\n","        train_acc_loss = []\n","        train_acc_accuracy = []\n","        net.train()\n","        for feature, label in tmp_loader:\n","            # get cropping slices for label\n","            if output_slices is None:\n","                rand_tensor = torch.rand(feature.shape).to(device)\n","                net_output_shape = net(rand_tensor).shape\n","                output_slices = get_cropping(label.shape, net_output_shape)\n","            label = label[output_slices]\n","            label = label.type(dtype)\n","            label = label.to(device)\n","            feature = feature.to(device)\n","            loss_value, outputs = training_step(net, loss_fn, optimizer, feature, label)\n","            pbar.update(1)\n","            train_acc_loss.append(loss_value.cpu().detach().numpy())\n","            accuracy = calc_accuracy(outputs['pred'], label)\n","            train_acc_accuracy.append(float(accuracy.cpu().detach().numpy()))\n","        history['loss'].append(np.mean(train_acc_loss))\n","        history['binary_accuracy'].append(np.mean(train_acc_accuracy))\n","        \n","        # validation\n","        net.eval()\n","        tmp_val_loader = iter(val_loader)\n","        val_acc_loss = []\n","        val_acc_accuracy = []\n","        for feature, label in tmp_val_loader:\n","            label = label[output_slices]                    \n","            label = label.type(dtype)\n","            label = label.to(device)\n","            feature = feature.to(device)\n","            loss_value, outputs = training_step(net, loss_fn, optimizer, feature, label)\n","            val_acc_loss.append(loss_value.cpu().detach().numpy())\n","            accuracy = calc_accuracy(outputs['pred'], label)\n","            val_acc_accuracy.append(float(accuracy.cpu().detach().numpy()))\n","        history['val_loss'].append(np.mean(val_acc_loss))\n","        history['val_binary_accuracy'].append(np.mean(val_acc_accuracy))\n","        print(f'Epoch {epoch+start_epoch+1}, train-loss: {np.mean(train_acc_loss):.4f} - train_accuracy:{np.mean(train_acc_accuracy):.4f}'+\n","      f' - val_loss:{np.mean(val_acc_loss):.4f} -val_accuracy:{np.mean(val_acc_accuracy):.4f}')\n","    \n","    print('Finished Training')\n","    # save_model(epoch, net, optimizer, train_acc_loss, \"model_{}_{}.pth\".format(dataset, epoch)) # uncomment this line to save the model for submission\n","    \n","    return net, history, optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKkCB22t0Jgf","scrolled":true},"outputs":[],"source":["###########################################################################\n","# TODO:  train the net                       #                     \n","###########################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# define num_epochs, loss_function, learning_rate and optimizer\n","# call train()\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUzGbOXV0JgV"},"outputs":[],"source":["def running_mean(x, n):\n","    cumsum = np.cumsum(np.insert(x, 0, 0)) \n","    return (cumsum[n:] - cumsum[:-n]) / float(n)\n","\n","def plot_history(history):\n","    # plot training and validation loss and binary accuracy\n","    \n","    loss = running_mean(history['loss'], 9)\n","    val_loss = running_mean(history['val_loss'], 9)\n","    #epochs = len(history.history['loss'])\n","    epochs = len(loss)\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","    ax1.plot(range(0, epochs), loss , label='loss')\n","    ax1.plot(range(0, epochs), val_loss, label='val_loss')\n","    ax1.set_title('train and validation loss')\n","    ax1.legend(loc='upper right')\n","    \n","    acc = running_mean(history['binary_accuracy'], 9)\n","    val_acc = running_mean(history['val_binary_accuracy'], 9)\n","\n","    ax2.plot(range(0, epochs), acc, label='binary_accuracy')\n","    ax2.plot(range(0, epochs), val_acc, label='val_binary_accuracy')\n","    ax2.set_title('train and validation binary accuracy')\n","    ax2.legend(loc='lower right')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-N2y3540Jgg"},"outputs":[],"source":["# plot loss and accuracy\n","plot_history(history)"]},{"cell_type":"markdown","metadata":{"id":"MsTq-oqd0Jgg"},"source":["## Test and evaluate our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17PSbQbg0JgW"},"outputs":[],"source":["def show_predictions(raw, gt, pred):\n","    \n","    thresh = 0.9\n","    max_values = np.max(pred[:,0], axis=(1, 2))\n","    if np.any(max_values < thresh):\n","        print(\"Heads up: If prediction is below {} then the prediction map is shown.\".format(thresh))\n","        print(\"Max predictions: {}\".format(max_values))\n","    \n","    num_samples = pred.shape[0]\n","    fig, ax = plt.subplots(num_samples, 3, sharex=True, sharey=True, figsize=(12, num_samples * 4))\n","    for i in range(num_samples):\n","        ax[i, 0].imshow(raw[i,0], aspect=\"auto\")\n","        ax[i, 1].imshow(gt[i,0], aspect=\"auto\")\n","        # check for prediction threshold\n","        if np.sum(max_values[i]) < thresh:\n","            ax[i, 2].imshow(pred[i,0], aspect=\"auto\")\n","        else:\n","            ax[i, 2].imshow(pred[i,0] >= thresh, aspect=\"auto\")\n","\n","    ax[0, 0].set_title(\"Input\")\n","    ax[0, 1].set_title(\"Ground truth\")\n","    ax[0, 2].set_title(\"Prediction\")\n","    fig.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5p374tUo0Jgg"},"outputs":[],"source":["# predict the test set\n","def predict(net, test_loader):\n","    net.eval()\n","    predictions = []\n","    acc_accuracy = []\n","    output_slices = None\n","    for image, label in test_loader:\n","        # get cropping slices for label\n","        if output_slices is None:\n","            rand_tensor = torch.rand(image.shape).to(device)\n","            net_output_shape = net(rand_tensor).shape\n","            output_slices = get_cropping(label.shape, net_output_shape)\n","        image = image.to(device)\n","        label = label[output_slices]\n","        label = label.to(device)\n","        pred = net(image)\n","        pred = activation(pred)\n","        accuracy = calc_accuracy(pred, label)\n","        acc_accuracy.append(float(accuracy.cpu().detach().numpy()))\n","        image = np.squeeze(image.cpu())\n","        pred = np.squeeze(pred.cpu().detach().numpy(),0)\n","        predictions.append(pred)\n","    return predictions, float(np.mean(acc_accuracy))\n","    \n","# plot predicted results\n","predictions, mean_accuracy = predict(net, test_loader)\n","predictions = np.stack(predictions, axis=0)\n","print('Accuracy: {:.3f}'.format(mean_accuracy))\n","\n","x_test = []\n","y_test = []\n","for tmp_x, tmp_y in data_test:\n","    x_test.append(tmp_x)\n","    y_test.append(tmp_y)\n","x_test = np.expand_dims(np.concatenate(x_test), axis=1)\n","y_test = np.expand_dims(np.concatenate(y_test), axis=1)\n","\n","show_predictions(x_test, y_test, predictions)"]},{"cell_type":"markdown","metadata":{"id":"IjY1oSIP0Jgi"},"source":["The training of the networks depend on many hyperparameters such as\n","- network architecture: #layers, #fmaps\n","- batch size, learning rate\n","- number and distribution of the training samples\n","\n","You can play and see how these settings influence the learning curve.\n","\n","![image.png](https://drive.google.com/uc?export=view&id=18KqN2SHcjVk0EhQtG7JI8SerbpMtBldZ)"]},{"cell_type":"markdown","metadata":{"id":"vVvYEVUAmGWF"},"source":["# todo: (Effective) Receptive Field of View\n","\n","The number of convolutions and the depth of the U-Net are the major factors in determining the \n","receptive field of the network. The term is borrowed from biology where it describes the \"portion of sensory space that can elicit neuronal responses when stimulated\" (wikipedia). Each output pixel can look at/depends on an input patch with that diameter centered at its position.\n","Based on this patch, the network has to be able to make a decision about the prediction for the respective pixel.\n","Yet larger sizes increase the computation time significantly.\n","\n","This paper gives a short introduction into the computation: [What are the Receptive, Effective Receptive, and Projective Fields of Neurons in Convolutional Neural Networks?](https://arxiv.org/pdf/1705.07049.pdf).\n","\n","For more detail you can look into [Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/).\n","\n","The effective receptive field of your network depends on the kind and number of layers.\n","Some layers have no impact on it, e.g. ReLu as it only works on individual pixels.\n","Some layers have a one-time effect on it, e.g. convolutional layers, as they work on a local patch of pixels.\n","A convolutional kernel with a size of 3 looks at the pixel itself and the direct neighbor on each side, it thus increases the receptive field by 2 (per dimension).\n","Some layers have a permanent effect on it, e.g. pooling. For example, 2x max-pooling (with a stride of two) reduces the image size by a factor of two and selects the maximum value of each block of size 2.\n","However, all following convolutional layers now operate on the pooled features.\n","Thus the pixels the kernel covers each correspond to two pixels from before the pooling layer.\n","A convolutional kernel with size 3 thus increases the receptive field not by 2 but by 2 * size of pooling layer * number of pooling layers before it.\n","\n","Think about how the other layers in your network influence the field of view of your network.\n","\n","Define a function to compute the receptive field of your U-Net.\n","Plot it onto one of the images from the datasets."]},{"cell_type":"code","source":["def plot_receptive_field(image, fov):\n","    image = np.squeeze(image)\n","    fig = plt.figure(figsize=(8, 8))\n","    plt.imshow(image, cmap='gray')\n","    xmin = image.shape[1]/2 - fov/2\n","    xmax = image.shape[1]/2 + fov/2\n","    ymin = image.shape[1]/2 - fov/2\n","    ymax = image.shape[1]/2 + fov/2\n","    plt.hlines(ymin, xmin, xmax, color=\"magenta\", lw=3)\n","    plt.hlines(ymax, xmin, xmax, color=\"magenta\", lw=3)\n","    plt.vlines(xmin, ymin, ymax, color=\"magenta\", lw=3)\n","    plt.vlines(xmax, ymin, ymax, color=\"magenta\", lw=3)\n","    plt.show()"],"metadata":{"id":"acUmPoFYLlR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cLsGWZEmGWF"},"outputs":[],"source":["inputs, labels = dataiter.next()\n","\n","###########################################################################\n","# TODO: Add a function to your U-Net class to compute its receptive field #\n","# then call it here to visualize it on an example image                   #\n","###########################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","fov = None\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################\n","print(fov)\n","plot_receptive_field(inputs, fov)"]},{"cell_type":"markdown","metadata":{"id":"WA7ZY9h5nDXR"},"source":["## Question 1\n","Compute and plot the receptive field for multiple different U-Net sizes.  \n","Hypothesize shortly what receptive field sizes might make sense for the kaggle nuclei dataset and why?  \n","(The important part here is to think about it and come up with arguments for your theories.  \n","You get points even if your ideas don't apply in this case)  \n","\n","################ START OF YOUR TEXT ################\n","\n","################ END OF YOUR TEXT ################\n"]},{"cell_type":"markdown","metadata":{"id":"5G0Yy0pbOlNm"},"source":["\n","# Instance Segmentation\n","\n","So far we were only interested in classes, what is background and foreground, \n","where are cells or person vs car. But in many cases we not only want to know\n","if a certain pixel belongs to a cell, but also to which cell.\n","\n","For isolated objects, this is trivial, all connected foreground pixels form\n","one instance, yet often instances are very close together or even overlapping.\n","Then we need to think a bit more how to formulate the loss for our network\n","and how to extract the instances from the predictions."]},{"cell_type":"markdown","metadata":{"id":"AHbYAufiOlNs"},"source":["Loss\n","-------\n","There are many different approaches to instance segmentation.\n","We will introduce three basic methods:\n","\n","### Three-class model ###\n","This is an extension of the basic foreground/background (or two-class) model.\n","In addition a third class is introduced: the boundary.\n","Even if two instances are touching, there is a boundary between them. This way they can be separated. \n","Instead of a single output (where an output of zero is one class and of one is the other class), the network outputs three values, one per class. And the loss function changes from binary to (sparse) categorical cross entropy.\n","\n","![three_class.png](https://drive.google.com/uc?export=view&id=1ORZTlHXJHKNFLf9U-PVraCNKLLgD-UON)\n","\n","### Distance Transform ###\n","The label for each pixel is the distance to the closest boundary. \n","The value within instances is negative and outside of instances is positive.\n","As the output is not a probability but an (in principle) unbounded scalar, the mean squared error loss function is used.\n","\n","![sdt.png](https://drive.google.com/uc?export=view&id=1NucFhe9qWvj26A7R7BUQhCFsktDIZrYF)\n","\n","\n","### Edge Affinities ###\n","Here we consider not just the pixel but also its direct neighbors (in 2D the left neighbor and the upper neighbor are sufficient, right and down are redundant with the next pixel's left and upper neighbor).\n","Imagine there is an edge between two pixels if they are in the same class and no edge if not. If we then take all pixels that are directly and indirectly connected by edges, we get an instance. The network predicts the probability that there is an edge, this is called affinity.\n","As we are considering two neighbors per pixel, our network needs two outputs and as the output is a probability, we are using binary cross entropy\n","\n","![affinities.png](https://drive.google.com/uc?export=view&id=1qRxqyFLmVqDx4EP4aQPaJbLZKsaV5OLq)\n","\n","### Metric Learning ###\n","In metric learning your model learns to predict an embedding vector for each pixel. These embedding vectors are learned such that vectors from pixels belonging to the same instance are similar to each other and dissimilar to the embedding vectors of other instances and the background. It can also be thought of as learning a false coloring where each instance is colored with a unique but arbitrary color.  \n","![metric_learning.png](https://drive.google.com/uc?export=view&id=1ynjUUcsnADX2VltNg26RtQzLYl7S0cDQ)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TrI932ndovp0"},"source":["## todo: Define activation and loss functions\n","We have 5 types of labels, corresponding to 5 **prediction_types** below.\n","\n","For each case, we should define the corresponding output channel numbers, final activation layer, criterion(loss function) and dtye(the data type of the label).It would be clear to fill in these conditions after you look through the part of code about how we define training process.\n","\n","Please fill in the missing code and uncomment one of the **prediction_type** to start your training. Please consider the discriminative loss from utils for the metric learning approach."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itBNpeFsovp1"},"outputs":[],"source":["from utils.disc_loss import DiscriminativeLoss\n","\n","###########################################################################\n","# TODO: Uncomment the prediction_type (and corresponding conditions)      #\n","#       you would like to use for this exercice                           #\n","###########################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","# Uncomment to choose one\n","\n","#prediction_type = \"two_class\" # same as fg/bg\n","#prediction_type = \"three_class\"\n","#prediction_type = \"affinities\"\n","#prediction_type = \"sdt\"\n","#prediction_type = \"metric_learning\"\n","\n","if prediction_type == \"two_class\":\n","    out_channels = \n","    activation =\n","    loss_fn = \n","    dtype = \n","elif prediction_type == \"affinities\":\n","    out_channels = \n","    activation =\n","    loss_fn = \n","    dtype = \n","elif prediction_type == \"sdt\":\n","    out_channels =\n","    activation = \n","    loss_fn = \n","    dtype = \n","elif prediction_type == \"three_class\":\n","    out_channels = \n","    activation = \n","    loss_fn =\n","    dtype = \n","elif prediction_type == \"metric_learning\":\n","    out_channels = \n","    activation = \n","    loss_fn = \n","    dtype = \n","else:\n","    raise RuntimeError(\"invalid prediction type\")\n","    \n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################"]},{"cell_type":"markdown","metadata":{"id":"CPSW17KmOlNu"},"source":["Create our input datasets, ground truth labels are chosen depending on the type:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VuVxLBhOlNv"},"outputs":[],"source":["# make datasets\n","dataset = \"dsb2018\"\n","padding_size = 12  # you might need to change this, e.g. 10 for valid padding, 12 for same padding\n","batch_size = 4\n","\n","data_train = KaggleDSB_dataset(dataset, \"train\", prediction_type=prediction_type, padding_size=padding_size)\n","data_val = KaggleDSB_dataset(dataset, \"val\", prediction_type=prediction_type, padding_size=padding_size)\n","data_test = KaggleDSB_dataset(dataset, \"test\", prediction_type=prediction_type, padding_size=padding_size)\n","# make dataloaders\n","train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n","val_loader = DataLoader(data_val, batch_size=1, pin_memory=True)\n","test_loader = DataLoader(data_test, batch_size=1)"]},{"cell_type":"markdown","metadata":{"id":"faMWkovLOlNw"},"source":["Let's have a look at some of the raw data and labels:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWP4xCcmOlNx"},"outputs":[],"source":["# repeatedly execute this cell to get different images\n","for image, label in data_train:\n","    break\n","\n","label = np.squeeze(label, 0)\n","if prediction_type == \"affinities\":\n","    label = label[0] + label[1]\n","\n","fig=plt.figure(figsize=(12, 8))\n","fig.add_subplot(1, 2, 1)\n","plt.imshow(np.squeeze(image), cmap='gray')\n","fig.add_subplot(1, 2, 2)\n","plt.imshow(np.squeeze(label), cmap='gist_earth')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Oi4Bdm-bJvYy"},"source":["##todo: Define our U-Net\n","\n","As before, we define our neural network architecture and can choose the depth and number of feature maps at the first convolution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I68uthIrJvYz"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","###########################################################################\n","# TODO: Define the net and uncomment the following code                   #\n","# Please define a UNet which use same padding                             #\n","###########################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = net.to(device)\n","summary(net, (1, 384, 384)) # e.g. (1, 380, 380) for valid padding, (1, 384, 384) for same padding"]},{"cell_type":"markdown","metadata":{"id":"Nk6vI6hAOlN0"},"source":["## todo: Training\n","\n","Before we start training, we have to compile the network and set the optimizer (try playing with the learning rate, a higher learning rate can lead to faster training, but also to divergence or lower performance).\n","\n","To visualize our results we now use Tensorboard. This is a very useful extension for your browser that let's you look into networks computational graph and the weights and metrics over time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npfIvMJPOlN0"},"outputs":[],"source":["def training_step(model, loss_fn, optimizer, feature, label, prediction_type):\n","    # speedup version of setting gradients to zero\n","    for param in model.parameters():\n","        param.grad = None\n","    # forward\n","    logits = model(feature) # B x C x H x W\n","    shape_dif = np.array(label.shape[-2:]) - np.array(logits.shape[-2:])\n","    if np.sum(shape_dif)>0:\n","        label = label[:,:,shape_dif[0]//2:-shape_dif[0]//2,shape_dif[0]//2:-shape_dif[0]//2]        \n","    if prediction_type == \"three_class\":\n","        label=torch.squeeze(label,1) #label.shape=[N,H,W]\n","    loss_value = loss_fn(input=logits, target=label)  #logits.shape=[N,C,H,W] label.shape=[N,H,W]\n","    # backward if training mode\n","    if net.training:\n","        loss_value.backward()\n","        optimizer.step()\n","    if activation is not None:\n","        output = activation(logits)\n","    else:\n","        output = logits\n","    outputs = {\n","        'pred': output,\n","        'logits': logits,\n","    }\n","    return loss_value, outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_PNIU-jOlN0","scrolled":false},"outputs":[],"source":["training_steps = 2000 # change to whatever works for you\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","writer = SummaryWriter(logdir)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","###########################################################################\n","# TODO: put the model and the loss function to device and define the      #\n","# optimizer with its learning rate                                        #\n","###########################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"degFvUK9JvY5"},"outputs":[],"source":["# set flags\n","net.train() \n","loss_fn.train()\n","step = 0\n","\n","# this might take ~10 ish minutes\n","with tqdm(total=training_steps) as pbar:\n","    while step < training_steps:\n","        # reset data loader to get random augmentations\n","        np.random.seed()\n","        tmp_loader = iter(train_loader)\n","        for feature, label in tmp_loader:\n","            label = label.type(dtype)\n","            label = label.to(device)\n","            feature = feature.to(device)\n","            loss_value, pred = training_step(net, loss_fn, optimizer, feature, label, prediction_type)\n","            writer.add_scalar('loss',loss_value.cpu().detach().numpy(),step)\n","            step += 1\n","            pbar.update(1)\n","            if step % 100 == 0:\n","                net.eval()\n","                tmp_val_loader = iter(val_loader)\n","                acc_loss = []\n","                for feature, label in tmp_val_loader:                    \n","                    label = label.type(dtype)\n","                    label = label.to(device)\n","                    feature = feature.to(device)\n","                    loss_value, _ = training_step(net, loss_fn, optimizer, feature, label, prediction_type)\n","                    acc_loss.append(loss_value.cpu().detach().numpy())\n","                writer.add_scalar('val_loss',np.mean(acc_loss),step)\n","                net.train()\n","# todo: save model ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78tq1yX2OlN1","scrolled":false},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir logs\n","\n","#or run:\n","#!tensorboard --logdir=runs \n","#to view in separate window"]},{"cell_type":"markdown","metadata":{"id":"GrsSO59gOlN1"},"source":["## Postprocessing\n","\n","In contrast to the semantic segmentation the postprocessing to extract the final segmentation is a bit more involved and consists of x steps for the two class, three class, sdt and affinity models:\n","- based on the prediction we define a surface\n","- we extract the maxima from this surface\n","- we use the maxima as seeds in an off-the-shelf watershed algorithm\n","- and mask the result with the foreground.\n","The foreground areas covered by the watershed from each seed point correspond to the instances.\n","The resulting instances are then matched to the ground truth instances (at least 50% overlap) to get our final score (averaged over all instances and all test images)\n","\n","For the metric learning model, the post-processing is a bit different. The embeddings are clustered with the mean shift algorithm and the clusters are numbered. You can think of this as clustering pixels by their color, such that the pixels that belong to one uniquely colored instance end up in one cluster and get the same number assigned. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyU505GiOlN2","scrolled":false},"outputs":[],"source":["%reload_ext autoreload\n","%autoreload 2\n","from utils.label import *\n","from utils.evaluate import *\n","\n","# set flag\n","net.eval()\n","# set hyperparameters\n","# thresholds have to be tuned after training on the validation set\n","if prediction_type == \"two_class\":\n","    fg_thresh = 0.7\n","    seed_thresh = 0.8\n","elif prediction_type == \"three_class\":\n","    fg_thresh = 0.5\n","    seed_thresh = 0.6\n","elif prediction_type == \"sdt\":\n","    fg_thresh = 0.0\n","    seed_thresh = -0.12\n","elif prediction_type == \"affinities\":\n","    fg_thresh = 0.9\n","    seed_thresh = 0.99\n","elif prediction_type == \"metric_learning\":\n","    fg_thresh = 0.5\n","    seed_thresh = None\n","    \n","def unpad(pred, padding_size):\n","    return pred[padding_size:-padding_size,padding_size:-padding_size]\n","\n","avg = 0.0\n","\n","input_size = None\n","output_size = None\n","pad_top_left = None\n","\n","for idx, (image, gt_labels) in enumerate(test_loader):\n","    # get network output size\n","    if output_size is None:\n","      input_size = image.shape # [-2:]\n","      rand_tensor = torch.rand(image.shape).to(device)\n","      output_size = net(rand_tensor).shape\n","      pad_top_left = (input_size[2] - output_size[2]) // 2  # assume size with shape B x C x H x W\n","    \n","    image = image.to(device)\n","    if pad_top_left == 0:\n","        pred = net(image)\n","    else:\n","      # use tile-and-stitch if valid padding\n","      image_padded = F.pad(image, (pad_top_left, 300, pad_top_left, 300))\n","      B, C, H, W = image.shape\n","      patched_pred = torch.zeros(B, output_size[1], image.shape[2] + 200, image.shape[3] + 200)\n","      for h in range(0, H, output_size[2]):\n","          for w in range(0, W, output_size[3]):\n","              image_tmp = image_padded[:, :, h:h+input_size[2], w:w+input_size[3]]\n","              pred = net(image_tmp)\n","              patched_pred[:, :, h:h+pred.shape[2], w:w+pred.shape[3]] = pred\n","      pred = patched_pred[:,:, 0:input_size[2], 0:input_size[3]]\n","    \n","    image = np.squeeze(image.cpu())\n","    gt_labels = np.squeeze(gt_labels)\n","    pred = np.squeeze(pred.cpu().detach().numpy(), 0)\n","    if padding_size and padding_size > 0:\n","        pred = unpad(np.transpose(pred,(1,2,0)), padding_size)\n","        pred = np.transpose(pred,(2,0,1))\n","    if prediction_type == \"affinities\":\n","        gt_labels = gt_labels[0] + gt_labels[1]\n","    \n","    labelling, surface = label(pred, prediction_type, fg_thresh=fg_thresh, seed_thresh=seed_thresh)\n","    ap, precision, recall, tp, fp, fn = evaluate(labelling, data_test.get_filename(idx))\n","    avg += ap\n","    print(np.min(surface), np.max(surface))\n","    labelling = labelling.astype(np.uint8)\n","    print(\"average precision: {}, precision: {}, recall: {}\".format(ap, precision, recall))\n","    print(\"true positives: {}, false positives: {}, false negatives: {}\".format(tp, fp, fn))\n","    if prediction_type == \"metric_learning\":\n","        surface = surface + np.abs(np.min(surface, axis=(1,2)))[:,np.newaxis,np.newaxis]\n","        surface /= np.max(surface, axis=(1,2))[:, np.newaxis, np.newaxis]\n","        surface = np.transpose(surface, (1,2,0))\n","\n","    fig=plt.figure(figsize=(16, 8))\n","    ax = fig.add_subplot(1, 4, 1)\n","    ax.set_title(\"raw\")\n","   \n","    image=unpad(image,(image.shape[-1]-labelling.shape[-1])//2)\n","    plt.imshow(np.squeeze(image))\n","    ax = fig.add_subplot(1, 4, 2)\n","    ax.set_title(\"gt labels\")\n","    gt_labels=unpad(gt_labels,(gt_labels.shape[-1]-labelling.shape[-1])//2)\n","    plt.imshow(np.squeeze(1.0-gt_labels))\n","    \n","    ax = fig.add_subplot(1, 4, 3)\n","    ax.set_title(\"prediction\")\n","    plt.imshow(np.squeeze(1.0-surface))\n","    ax = fig.add_subplot(1, 4, 4)\n","    ax.set_title(\"pred segmentation\")\n","    plt.imshow(np.squeeze(labelling), cmap=rand_cmap, interpolation=\"none\")\n","\n","    plt.show()\n","avg /= (idx+1)\n","print(\"average precision on test set: {}\".format(avg))"]},{"cell_type":"markdown","source":["## todo: Class-intern benchmark\n","Try different methods, hyperparameters, and see how you can improve your results. Write down your best average precision on the test set with the corresponding network settings.\n","\n","\n","################ START OF YOUR TEXT ################\n","\n","################ END OF YOUR TEXT ################"],"metadata":{"id":"UA-gkfldUGJ1"}},{"cell_type":"code","source":["\n"],"metadata":{"id":"u7aie8CiU1CF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"U0MpdCcDtXbj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question 2 \n","\n","Do you have further ideas how to improve our instance segmentation approach (e.g. with changes regarding to architecture, loss, etc.)?\n","\n","\n","################ START OF YOUR TEXT ################\n","\n","################ END OF YOUR TEXT ################"],"metadata":{"id":"tncBj46StjC0"}},{"cell_type":"code","source":[""],"metadata":{"id":"WrNadi6UtxZ8"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"image_segmentation.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}
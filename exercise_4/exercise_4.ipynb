{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this exercise you'll implement a fully-connected neural network and the computations necessary for the backpropagation algorithm to optimize its parameters. Before starting on the programming exercise, we strongly recommend watching the video lectures and going over the theoretical exercise sheet first. All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook.\n",
    "\n",
    "CAUTION: You need python version >= 3.4!\n",
    "\n",
    "## Grading\n",
    "\n",
    "For this programming exercise, there are 70 points in total.\n",
    "\n",
    "**Required Exercises**\n",
    "\n",
    "| Section | Part                                          |Submitted Function                     | Points \n",
    "|---------|:-                                             |:-                                     | :-:    \n",
    "| 1       | [Implementing a Fully-Connected Neural Net](#fcnn) | [`affine_forward`](#affine_forward)    |  5  \n",
    "|         |                                               | [`relu_forward`](#relu_forward) |  5\n",
    "|         |                                               | [`relu_backward`](#relu_backward) |  5\n",
    "|         |                                               | [`affine_backward`](#affine_backward) |  5\n",
    "| 2       | [Softmax classifier](#Softmax_classifier)     | [`cross_entropy_loss`](#cross_entropy_loss) |  10\n",
    "|         |                                               | [`TwoLayerNet`](#TwoLayerNet) |  10\n",
    "| 3       | [Qualitative Analysis](#Qualitative_Analysis)     | [`50_best`](#50_best) |  10\n",
    "|         |                                               | [`50_worst`](#50_worst) |  10\n",
    "|         |                                               | [`q3`](#q3) |  5\n",
    "|         |                                               | [`q4`](#q4) |  5\n",
    "|         | Total Points                                  |                                       | 70\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fCEDCU_qrC0"
   },
   "source": [
    "## Google Colab\n",
    "\n",
    "Colaboratory, or \"Colab\" for short, allows you to write and execute Python notebooks in your browser, with \n",
    "- close to zero configuration required\n",
    "- Free access to GPUs (Nvidia Tesla K80)\n",
    "- Easy sharing and collaborative work on a single file.\n",
    "\n",
    "Just like local python notebooks, this file consists of (markdown) text cells and python code cells. You can add and delete cells and move them around in this document. You can change the runtime type from CPU to GPU (`Runtime-> Change runtime type -> Hardware Accelerator -> GPU`) to speed up your computations when using a GPU-capable machine learning library. You can import your python libraries, load data from the web and even install new libraries on the virtual machine by typing `!pip install <libraryname>` , the exclamation mark lets you escape the python environment and type on the commandline. \n",
    "\n",
    "**Requirements** To use Colab, you must have a Google account with an associated Google Drive.\n",
    "\n",
    "**Workflow** For the assignments, you can keep on working locally with the python notebooks that we provide for the practical. But we recommend to use colab, because it offers a nice way of working together as a group on the practical assignments. Furthermore the free GPU will be useful for future assignments that require more computational ressources. \n",
    "\n",
    "**Limitations** Ressources on colab are not guaranteed and therefore there might be times where some ressources cannot get allocated. If you're idle for 90 minutes or your connection time exceeds the maximum of 12 hours, the colab virtual machine will disconnect. This means that unsaved progress such as model parameters are lost. \n",
    "\n",
    "**Assignment submission** Just download the notebook at `File->Download .ipynb` and hand it in as usual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j88AMjjYliG9"
   },
   "source": [
    "<a id=\"Image_classification\"></a>\n",
    "## Image classification\n",
    "The task for this practical is to build a multi-class image classification model. The dataset we'll work with is the CIFAR10 image classification benchmark. It consists of 50,000 training images (each in $32 \\times 32$ resolution with RGB color channels). Each pixel is represented as three floating point numbers, indicating the color intensity in the respective color channel. Each image is labeled with integers ranging from 0 to 9 (0-9), indicating the class of the image content. The 10 classes and according example images from the dataset are shown in the image below. The dataset also consists of a test dataset, that consists of another 10,000 images and their labels. \n",
    "\n",
    "![Cifar10](https://pytorch.org/tutorials/_images/cifar10.png)\n",
    "\n",
    "We further split the training images into 45,000 training samples and 5,000 validation samples. The network will just be trained on the training dataset and we use the validation data to check during training if the network generalizes well to new data. In future assignments, the validation dataset is used to tweak the hyperparameters of the model. The test data is not used until the very end of the process, to evaluate the models generalization ability on prior unseen data (which serves as a proxy for deploying the model in the real world).  \n",
    "\n",
    "You can load the complete dataset into memory by just running the next cell. \n",
    "For loading the dataset, we used a convenience function from the package `tensorflow`. On colab, this package is already installed, on your local machine you might have to install it first. You can install tensorflow via pip or via Anaconda (which is often easier). However you don't need to install it: you can also just download the data [here](https://www.cs.toronto.edu/~kriz/cifar.html) and edit the function `get_CIFAR10_data` below. In this case, make sure that the function returns the data in these dimensions:\n",
    "\n",
    "```python\n",
    "('X_train: ', (45000, 3, 32, 32))\n",
    "('y_train: ', (45000,))\n",
    "('X_val: ', (5000, 3, 32, 32))\n",
    "('y_val: ', (5000,))\n",
    "('X_test: ', (10000, 3, 32, 32))\n",
    "('y_test: ', (10000,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ZZGXLSZordIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (45000, 3, 32, 32))\n",
      "('y_train: ', (45000,))\n",
      "('X_val: ', (5000, 3, 32, 32))\n",
      "('y_val: ', (5000,))\n",
      "('X_test: ', (10000, 3, 32, 32))\n",
      "('y_test: ', (10000,))\n"
     ]
    }
   ],
   "source": [
    "# load utils for this practical\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "\n",
    "#Load CIFAR10 data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "def get_CIFAR10_data(\n",
    "    num_training=45000, num_validation=5000, num_test=10000, subtract_mean=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    (train, test) = cifar10.load_data()\n",
    "    X_train, y_train = train\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test, y_test = test\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": np.squeeze(y_train),\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": np.squeeze(y_val),\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": np.squeeze(y_test),\n",
    "    }\n",
    "\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wdp9g2-ymd5A"
   },
   "source": [
    "<a id=\"fcnn\"></a>\n",
    "# 1. Implementing a Fully-Connected Neural Net\n",
    "We want to implement a neural network with at least one hidden layer, like the one below. It has 2 layers besides the input layer - , a hidden layer and an output layer. \n",
    "\n",
    "## 1.1 Forward pass\n",
    "![Network architecture](https://i.imgur.com/yv1FR86.png)  \n",
    "\n",
    "The forward pass of the network is denoted in green. Recall that our inputs are pixel values of RGB images. Since the images are of size $32 \\times 32 \\times 3$, this gives us 3072 input features for the first layer. Therefore, the weight vector of a single neuron $j$ in layler 2 ( $w^{(1)}_j)$ has 3072 values and a single bias $b^{(1)}_j$. By applying the activation function $g(.)$ on the pre-activations $z^{(2)}$, we get the activations of the hidden layer $a^{(2)}=g(z^{(2)})$. The same applies for the output layer which takes the activations of the preceeding layer $a^{(2)}$ as the input, weights with $w^{(2)}$, adds bias $b^{(2)}$ and uses the activation function to arrive at $a^{(3)}$. The number of neurons in the output layer corresponds to the number of classes in the classification problem, so for CIFAR10 it'll be 10 output neurons. In our case $a^{(3)}$ holds the logits of the hypothesis $h_\\theta(x)$. These will be used in a loss function $L_\\theta$ to assess the predictions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNdYhTPof7Op"
   },
   "source": [
    "### 1.1.1 Affine layer: forward\n",
    "Now please implement the function `affine forward` below, which calculates pre-activations $z^{(l)} = w^{(l-1)}x + b^{(l)}$  and test your implementation with the provided test cases.\n",
    "<a id=\"affine_forward\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7emza5V_nC0X"
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    N = x.shape[0]\n",
    "    D, M = w.shape\n",
    "\n",
    "    xFlatt = x.reshape((N, D))\n",
    "    # one_xFlatt = np.concatenate([np.ones((N, 1)), xFlatt], axis=1)\n",
    "    # theta = np.concatenate([b.reshape(1,M), w], axis=0)\n",
    "    # out = np.dot(one_xFlatt, theta)\n",
    "\n",
    "    out = np.dot(xFlatt, w) + b\n",
    "\n",
    "    # out = []\n",
    "    # for batch in x:\n",
    "    #     flatt = batch.flatten()\n",
    "    #     out.append(np.dot(w.T, flatt) + b)\n",
    "    # out = np.array(out)\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHAVUeG0tpbT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.769849468192957e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4CPiv9c0IU2"
   },
   "source": [
    "### 1.1.2 ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation. The ReLU activation function is just $g(z) =  \\begin{cases}\n",
    "    z, & \\text{for } z > 0 \\\\\n",
    "    0, & \\text{for } z \\leq 0 \\\\\n",
    "  \\end{cases}$\n",
    "<a id=\"relu_forward\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTy1NIny0f8T"
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    out = np.maximum(x, 0)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTvE_rG90RKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1fKMa-euLrR"
   },
   "source": [
    "## 1.2 Backward pass\n",
    "The backward pass of the network is denoted in red. The general idea of the backpropagation of errors algorithm is to calculate partial derivatives of the loss $L_\\theta$ with respect to the (pre-)activations of every layer in the network. Then the gradient descent algorithm is used to calculate the partial derivatives of the pre-activations with respect to the weights and biases.  \n",
    "\n",
    "![Network architecture](https://i.imgur.com/SHe3l14.png) \n",
    "\n",
    "This procedure starts with the derivative of the loss with respect to the hypothesis $h_\\theta(x)=a^{(3)}$, so $\\frac{\\partial L_{\\theta}}{\\partial a^{(3)}}$, which will be calculated by the function `cross_entropy_loss`. Then the derivative of the activation function $\\frac{\\partial a^{(l)}}{z^{(l)}}$ with respect to the pre-activations is calculated by the function `relu_backward` and we get $\\delta^{(l)}=\\frac{\\partial L_{\\theta}} {\\partial a^{(l)}}\\frac{\\partial a^{(l)}}{z^{(l)}}$. Finally, the function `affine_backward` computes the derivatives `dw`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial w^{(l-1)}}$, `db`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial b^{(l-1)}}$ and `dx`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial a^{(l-1)}}$, which are needed to update the parameters of the model. \n",
    "\n",
    "Hint: When figuring out the derivatives needed for the back-propagation algorithm, you can always think about them in terms of the circuit notation used in the stanford cs231n course to check if they are plausible to you:\n",
    "\n",
    "![Circuit](https://i.imgur.com/YN6z5gK.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkb2wo0S0XHZ"
   },
   "source": [
    "### 1.2.1 ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using gradient checking.\n",
    "<a id=\"relu_backward\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gKodVEu0iml"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    dx = (x >= 0) * dout\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8x9mv7Eh8Zf"
   },
   "source": [
    "### Small excursion: Numeric gradient checking\n",
    "Numeric gradient checking calculates a numerical approximation of the gradient by adding and substracting a small $\\epsilon$ from the variable of interest. When we want to check the gradient of the weights $\\theta$, we assume\n",
    "\n",
    "$ \\theta^{(i+)} = \\theta + \\begin{bmatrix} 0 \\\\  \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\  0 \\end{bmatrix}$ and $ \\theta^{(i-)} = \\theta - \\begin{bmatrix} 0 \\\\  \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\  0 \\end{bmatrix}$\n",
    "\n",
    "So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i^{th}$ element has been incremented by $\\epsilon$. Similarly, $\\theta^{(i−)}$ is the corresponding vector with the $i^{th}$ element decreased by $\\epsilon$. You can now numerically verify $\\frac{\\partial L_{\\theta}}{\\partial \\theta}$’s correctness by checking, for each $i$, that:\n",
    "\n",
    "$$ \\frac{\\partial L_{\\theta}^i}{\\partial \\theta} \\approx \\frac{L_{\\theta^{(i+)}} - L_{\\theta^{(i-)}}}{2\\epsilon} $$\n",
    "\n",
    "The degree to which these two values should approximate each other will depend on the details of $L_{\\theta}$. But assuming $\\epsilon = 10^{-8}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 8 significant digits (and often many more). This is already implemented in `eval_numerical_gradient_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjUg2SW40cEs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756349136310288e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INhrjy9AkVao"
   },
   "source": [
    "###  1.2.2 Affine layer: backward\n",
    "\n",
    "You've already implemented `relu_backward`, and this function gets it's output $\\frac{\\partial L_\\theta}{\\partial z^{(l)}}$ as the input, together with the cached variables from the forward pass $a^{(l-1)}$, $w^{(l-1)}$ and $b^{(l-1)}$. It computes the derivatives `dw`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial w^{(l-1)}}$, `db`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial b^{(l-1)}}$ and `dx`$=\\frac{\\partial L_\\theta}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial a^{(l-1)}}$\n",
    "\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking.\n",
    "<a id=\"affine_backward\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8D1btZYFtueV"
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    dx = []\n",
    "    dw = []\n",
    "    db = []\n",
    "\n",
    "    N = x.shape[0]\n",
    "    D, M = w.shape\n",
    "\n",
    "    dx = np.dot(dout,  w.T).reshape(x.shape)\n",
    "\n",
    "    xFlatt = x.reshape((N, D))\n",
    "    # one_xFlatt = np.concatenate([np.ones((N, 1)), xFlatt], axis=1)\n",
    "    # derivates = np.dot(one_xFlatt.T, dout)\n",
    "    # db = derivates[0]\n",
    "    # dw = derivates[1:]\n",
    "\n",
    "    dw = np.dot(xFlatt.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-wlQRymtxHh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  5.399100368651805e-11\n",
      "dw error:  9.904211865398145e-11\n",
      "db error:  2.4122867568119087e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNM22Pg63b25"
   },
   "source": [
    "## 1.3 \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define convenience layers in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oP13fBKQ3Xsg"
   },
   "outputs": [],
   "source": [
    "def affine_relu_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Convenience layer that performs an affine transform followed by a ReLU\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the affine-relu convenience layer\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_NEfZH93iJj"
   },
   "source": [
    "Let's check numerically the gradient backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0qTZPGG3NH3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error:  2.299579177309368e-11\n",
      "dw error:  8.162011105764925e-11\n",
      "db error:  7.826724021458994e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clw3O-DX4Fs1"
   },
   "source": [
    "<a id=\"Softmax_classifier\"></a>\n",
    "# 2. Softmax classifier\n",
    "The softmax classifier is widely used in multi-class classification probems. You've implemented a binary Logistic Regression classifier in the first assignment, the Softmax classifier is its generalization to multiple classes. Unlike hinge-loss classifiers which treat the outputs $f(x_i,W)$ as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation. In the Softmax classifier, the function mapping $f(x_i;W) = W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form:\n",
    "\n",
    "$ L_i = -log \\Big(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\Big)$ or equivalently $L_i =-f_{y_i} + log \\sum\\limits_j e^{f_j}$\n",
    "\n",
    "where we are using the notation $f_j$ to mean the j-th element of the vector of class scores $f$. As before, the full loss for the dataset is the mean of $L_i$ over all training examples. The function $f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ is called the softmax function: It takes a vector of arbitrary real-valued scores (in $z$) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate. Two really good motivations from a probabilistic and an information theoretic perspective are given on the [cs231n webpage](https://cs231n.github.io/linear-classify/).\n",
    "\n",
    "### Numerical stability\n",
    "When you’re writing code for computing the Softmax function in practice, the intermediate terms $e^{f_{y_i}}$ and $\\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant C and push it into the sum, we get the following (mathematically equivalent) expression:\n",
    "\n",
    "$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} = \\frac{C e^{f_{y_i}}}{C \\sum_j e^{f_j}} =  \\frac{e^{f_{y_i}+log(C)}}{\\sum_j e^{f_j + log(C)}}$\n",
    "\n",
    "We are free to choose the value of $C$. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for $C$ is to set $log(C)=−max_j(f_j)$. This simply states that we should shift the values inside the vector $f$ so that the highest value is zero. In code:\n",
    "```python\n",
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n",
    "```\n",
    "\n",
    "In the following, implement the `cross_entropy_loss` and test your implementation against the numerically approximated gradient.\n",
    "<a id=\"cross_entropy_loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "def cross_entropy_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C (C = Anzal de Klassen, y[i] muss gültige Klasse sein)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dx = np.zeros_like(x)\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dx. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability.                         #                                                           \n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    maxi =  np.max(x, axis=1)\n",
    "    xNorm = (x.T - maxi).T\n",
    "    sum = np.sum(np.exp(xNorm), axis=1)\n",
    "    softmax = (np.exp(xNorm).T / sum).T\n",
    "    temp = np.take(softmax, y)\n",
    "    print(softmax)\n",
    "    print(y)\n",
    "    print(np.choose(y, softmax.T))\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "    return loss, dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002144]\n",
      "[[0.25014871 0.25039126 0.24959553 0.24986451]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959553 0.24975236 0.25002144]\n",
      "[[0.25014496 0.25039251 0.24959678 0.24986576]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959678 0.24975236 0.25002144]\n",
      "[[0.2501462  0.25039376 0.24959553 0.24986451]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959553 0.24975236 0.25002144]\n",
      "[[0.25014746 0.25039001 0.24959678 0.24986576]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959678 0.24975236 0.25002144]\n",
      "[[0.25014621 0.25039126 0.24959803 0.24986451]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959803 0.24975236 0.25002144]\n",
      "[[0.25014746 0.25039251 0.24959428 0.24986576]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959428 0.24975236 0.25002144]\n",
      "[[0.25014621 0.25039126 0.24959553 0.24986701]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959553 0.24975236 0.25002144]\n",
      "[[0.25014746 0.25039251 0.24959678 0.24986326]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959678 0.24975236 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992919 0.24975174 0.24990793 0.25041114]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975174 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992545 0.24975298 0.24990918 0.25041239]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975298 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.2499267  0.24975423 0.24990793 0.25041114]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975423 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992794 0.24975049 0.24990918 0.25041239]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975049 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.2499267  0.24975174 0.24991043 0.25041114]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975174 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992794 0.24975298 0.24990668 0.25041239]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975298 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992669 0.24975173 0.24990793 0.25041364]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975173 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992795 0.24975298 0.24990918 0.25040988]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975298 0.25002144]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24976114 0.25002082 0.25002934 0.2501887 ]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002082]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975739 0.25002207 0.25003059 0.25018995]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002207]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975864 0.25002332 0.25002934 0.2501887 ]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002332]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975989 0.25001957 0.25003059 0.25018995]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25001957]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975864 0.25002082 0.25003184 0.2501887 ]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002082]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975989 0.25002207 0.25002809 0.25018995]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002207]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975864 0.25002082 0.25002934 0.25019121]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002082]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975989 0.25002207 0.25003059 0.25018745]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002207]\n",
      "[[0.25014683 0.25039188 0.24959615 0.24986513]\n",
      " [0.24992732 0.24975236 0.24990856 0.25041176]\n",
      " [0.24975926 0.25002144 0.25002996 0.25018933]]\n",
      "[2 1 1]\n",
      "[0.24959615 0.24975236 0.25002144]\n",
      "\n",
      "Testing cross_entropy_loss:\n",
      "loss:  0.0\n",
      "dx error:  0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# num_classes, num_inputs = 10, 50\n",
    "num_classes, num_inputs = 4, 3\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: cross_entropy_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = cross_entropy_loss(x, y)\n",
    "\n",
    "# Test cross_entropy_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting cross_entropy_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Two layer fully-connected Neural Network\n",
    "Now we have to put all the elements together. We want to construct a two layer network class `TwoLayerNet`, therefore we need to initialize two sets of weights and biases, one set for each of the two layers. This needs to be implemented in the `__init__` method. In the `loss` method, the parameters are unpacked and then the forward pass is done. It computes all activations throughout the network, including the hypothesis $h_\\theta(x)$. The hypothesis and the groundtruth values `y` are fed into the `cross_entropy_loss` which returns a loss value and the gradient $\\frac{\\partial L_\\theta}{\\partial a^{(3)}}$. Then the upstream derivatives `dx` and the partial derivatives with respect to the parameters of the last layer `dw` and `db` are calculated by the function `affine_relu_backward`. The upstream derivative `dx` is again fed into `affine_relu_backward` together with the cached variables from the first layer to calculate `dw` and `db` for the hidden layer. All of the gradients have to be stored into the dictionary `grads` and will be used by the `Solver` to update the parameters of the model.\n",
    "\n",
    "![Network architecture](https://i.imgur.com/Gl5bdaa.png) \n",
    "<a id=\"TwoLayerNet\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
    "    cross_entropy_loss that uses a modular layer design. We assume an input dimension\n",
    "    of D, a hidden dimension of H, and perform classification over C classes.\n",
    "\n",
    "    The architecure should be affine - relu - affine - relu.\n",
    "\n",
    "    Note that this class does not implement gradient descent; instead, it\n",
    "    will interact with a separate Solver object that is responsible for running\n",
    "    optimization.\n",
    "\n",
    "    The learnable parameters of the model are stored in the dictionary\n",
    "    self.params that maps parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=3 * 32 * 32,\n",
    "        hidden_dim=100,\n",
    "        num_classes=10,\n",
    "        weight_scale=1e-2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: An integer giving the size of the input\n",
    "        - hidden_dim: An integer giving the size of the hidden layer\n",
    "        - num_classes: An integer giving the number of classes to classify\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
    "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
    "        # standard deviation equal to weight_scale, and biases should be           #\n",
    "        # initialized to zero. All weights and biases should be stored in the      #\n",
    "        # dictionary self.params, with first layer weights                         #\n",
    "        # and biases using the keys 'W1' and 'b1' and second layer                 #\n",
    "        # weights and biases using the keys 'W2' and 'b2'.                         #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        scores = None\n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        W2 = self.params['W2']\n",
    "        b2 = self.params['b2']\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
    "        # class scores for X and storing them in the scores variable and computing #\n",
    "        # the cache that also gets stored.                                         #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # If y is None then we are in test mode so just return scores\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
    "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
    "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
    "        # self.params[k]. This means that dictionary grads needs to have the same  #\n",
    "        # keys as dictionary self.params                                           #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "print('Running numeric gradient check')\n",
    "loss, grads = model.loss(X, y)\n",
    "\n",
    "for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Train the network using the pre-built solver"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "lr = 1e-3\n",
    "hidden_dim = 100\n",
    "epochs = 20\n",
    "# build the network\n",
    "model = TwoLayerNet(hidden_dim = hidden_dim)\n",
    "# optimize\n",
    "solver = Solver(\n",
    "    model,\n",
    "    data,\n",
    "    optim_config={'learning_rate':lr}, \n",
    "    lr_decay=0.95,\n",
    "    num_epochs=epochs,\n",
    "    batch_size=100,\n",
    "    print_every=100,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### DO NOT EDIT THIS CELL ####\n",
    "solver.train()  \n",
    "solver.save_params(\"two_layer_net_params.pkl\")  # please hand in this file as well!\n",
    "#### DO NOT EDIT THIS CELL ####"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test set results\n",
    "#@markdown Visualization of the confusion matrix. On the vertical axis are the real labels,\n",
    "#@markdown on the horizontal axis are the predicted labels. \n",
    "#@markdown The overall accuracy of the model should be around ~0.45-0.5.\n",
    "\n",
    "acc, pred = solver.check_accuracy(data['X_test'], data['y_test'], num_samples=10000)\n",
    "y_pred = pred['y_pred']\n",
    "print('#########################################')\n",
    "print('Overall Accuracy on the test set: ', acc)\n",
    "print('#########################################')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "# sns heatmap because plt.matshow has bugs with setting ticklabels\n",
    "\n",
    "names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "conf_mat = confusion_matrix(y_true= data['y_test'], y_pred=y_pred)\n",
    "conf_mat = conf_mat/1000\n",
    "conf_mat = np.round(conf_mat,2)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "heatmap(conf_mat, cmap=\"Blues\", annot=True,annot_kws={\"size\": 10}, xticklabels=names, yticklabels=names)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 Browse the Test Set Predictions of your model\n",
    "Below you can insert different image indices `img_idx` and get the associated input image and a bar chart with ground truth and prediction. Your test set consists of 10,000 examples, so integers in the intervall $[0,9999]$ are valid."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_idx = 10\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def show_prediction(image_idx):\n",
    "    img = (np.transpose(data['X_test'][image_idx], [1,2,0])/255)+0.5\n",
    "    scores = np.transpose(pred['scores'])\n",
    "    scores = np.round(softmax(scores[image_idx]),2)\n",
    "    gt = np.zeros(10)\n",
    "    gt[data['y_test'][image_idx]]=1\n",
    "\n",
    "    plt.figure(figsize = (2,2))\n",
    "    plt.imshow(img, interpolation='lanczos')\n",
    "    names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    x_axis = np.arange(len(names))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    width = 0.5 \n",
    "    rects1 = ax.bar(x_axis-width/2, gt, 0.5, label='Groundtruth')\n",
    "    rects1 = ax.bar(x_axis+width/2, scores, 0.5, label='Prediction')\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(11))\n",
    "    ax.set_xticklabels(['']+names+[''])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "show_prediction(img_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Qualitative Analysis of Predictions\n",
    "<a id=\"Qualitative_Analysis\"></a>\n",
    "Last but not least, (1) find the 50 images with worst results and plot all these images, (2) find the 50 images with best results and plot them all, (3) speculate why the bad ones are not classified successfully (do they share some properties that well-classified images don't have?), and (4) brainstorm about potential modifications to the classifier which could help get the bad ones right. For (3) and (4), no answer is the only wrong answer :)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (1) find 50 worst images and plot them\n",
    "# (2) find 50 best images and plot them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<your answers to (3) and (4) here> \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ex3.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e53cc610943f2fb13dcd998ed36acf8abd25de5168bbf730423401635ca3cfac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('MLIA22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}